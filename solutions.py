import os
import io
import json
from groq import Groq
from pypdf import PdfReader
from pydantic import BaseModel, Field, ConfigDict
from openpyxl import Workbook
import streamlit as st
from dotenv import load_dotenv

# Optional: Loads .env if available for local development convenience
load_dotenv() 

LLM_MODEL = "llama-3.3-70b-versatile"

# ---------------------------------------------------------
# Function to initialize Groq client based on sidebar input
# ---------------------------------------------------------
def init_client(api_key: str):
    """Initializes and returns the Groq client."""
    if not api_key:
        return None, False

    try:
        # Client initialization with the user-provided key
        client = Groq(api_key=api_key)
        # Quick health check (optional, but good practice)
        client.models.list() 
        return client, True
    except:
        return None, False


# ---------------------------------------------------------
# Pydantic Models (Define the structure of the data)
# ---------------------------------------------------------
class ExtractedPair(BaseModel):
    """Represents a single row of data for the Excel output."""
    Key: str = Field(description="The determined key/label for the data point.")
    Value: str = Field(description="The exact original phrasing or fact from the PDF.")
    comment: str = Field(
        alias="Comment",
        description="Residual text from the source or supplementary context. MUST be '' if Key/Value is sufficient."
    )
    # Allows Pydantic to accept 'Comment' from the LLM JSON and convert it to the internal 'comment' attribute
    model_config = ConfigDict(populate_by_name=True)


class DocumentStructure(BaseModel):
    """The root model for the structured document output, enforcing the top-level key."""
    extracted_data: list[ExtractedPair] = Field(
        description="This array MUST contain ALL extracted Key:Value:Comment pairs."
    )


# ---------------------------------------------------------
# PDF Extraction
# ---------------------------------------------------------
@st.cache_data
def read_pdf_text(uploaded_file: io.BytesIO) -> str:
    """Reads all text content from an uploaded PDF file."""
    try:
        reader = PdfReader(uploaded_file)
        text = ""
        for i, page in enumerate(reader.pages):
            text += f"\n--- Page {i+1} ---\n"
            page_text = page.extract_text()
            text += page_text if page_text else "(No readable text on this page)"
        return text.strip()
    except Exception as e:
        # Remove cache on failure so the user can re-upload
        st.cache_data.clear()
        st.error(f"Error reading PDF: {e}")
        return ""


# ---------------------------------------------------------
# Prompt Creation (Optimized for JSON adherence)
# ---------------------------------------------------------
def generate_extraction_prompt(document_text: str) -> str:
    """
    Creates the optimized system prompt, using CoT and strict JSON instruction markers.
    """
    
    # 1. Generate the expected JSON schema structure from the Pydantic model
    # We strip out the outer 'DocumentStructure' wrapper in the example to simplify the prompt, 
    # but the instructions ensure the wrapper is included in the final output.
    schema_template = DocumentStructure.model_json_schema(by_alias=True)
    
    return f"""
    You are an advanced AI Data Structuring and Extraction Engine. Your task is to transform the provided
    unstructured document text into a structured JSON format.

    ## SCHEMA DEFINITION (CRITICAL):
    Your final output MUST be a JSON object that STRICTLY conforms to the following schema structure. 
    The single top-level key MUST be "extracted_data" containing an array of objects.

    {json.dumps(schema_template, indent=2)}

    ## PROCESSING METHODOLOGY (MANDATORY CHAIN-OF-THOUGHT):
    You MUST process the document by following these steps for every unique sentence or clause:
    1.  **IDENTIFY SOURCE:** Select one complete, logical sentence or clause from the document text.
    2.  **EXTRACT CORE VALUE:** From that source, extract the single, most important factual metric or phrase. This is the **Value**.
    3.  **DETERMINE KEY:** Create the most appropriate, concise, and logical **Key** for the fact identified in Step 2.
    4.  **CAPTURE CONTEXT/RESIDUAL:** Place any remaining associated text from the *original source sentence* that was NOT used in the Key or Value into the **Comment**. If the Key and Value capture the entire logical idea, the comment MUST be **EMPTY ("")**.

    ## STRICT RULES FOR FIDELITY:
    1.  **100% Capture:** All content MUST be captured across the three columns (Key, Value, Comment). Nothing is summarized or omitted.
    2.  **Language Preservation (CRITICAL):**
        * The **Value** MUST **Retain the exact original wording, sentence structure, and phrasing from the PDF**.
        * **Avoid paraphrasing unless required to form a clean key:value pair**.
        * **Do not introduce new information** or fabricate details.
    3.  **Final Output:** You MUST output ONLY the JSON object. DO NOT include any introductory text, markdown, or commentary outside of the JSON block.

    ## DOCUMENT TEXT TO BE PROCESSED:
    ---
    {document_text}
    ---
    """


# ---------------------------------------------------------
# LLM Extraction
# ---------------------------------------------------------
def extract_data_with_llm(client: Groq, document_text: str, progress_bar):
    system_prompt = generate_extraction_prompt(document_text)

    try:
        progress_bar.progress(60, text="60% - Sending data to Groq LLM...")

        chat_completion = client.chat.completions.create(
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": "BEGIN EXTRACTION: Return the structured JSON immediately."}
            ],
            model=LLM_MODEL,
            response_format={"type": "json_object"},
            temperature=0.0
        )

        progress_bar.progress(80, text="80% - Validating JSON...")

        json_string = chat_completion.choices[0].message.content
        data_model = DocumentStructure.model_validate_json(json_string)

        progress_bar.progress(95, text="95% - Success!")

        return data_model.extracted_data

    except Exception as e:
        progress_bar.empty()
        st.error(f"‚ùå LLM Error: {e}")
        st.caption("The LLM likely returned text/JSON that did not strictly contain the 'extracted_data' list. This is a model adherence issue.")
        return None


# ---------------------------------------------------------
# Excel File Creator
# ---------------------------------------------------------
def create_excel_bytes(extracted_data, progress_bar):
    progress_bar.progress(98, text="98% - Generating Excel...")

    wb = Workbook()
    ws = wb.active
    ws.append(["Key", "Value", "Comment"])

    for item in extracted_data:
        # model_dump(by_alias=True) ensures "Comment" is used as the key [cite: 1.1]
        row = item.model_dump(by_alias=True)
        ws.append([row["Key"], row["Value"], row["Comment"]])

    stream = io.BytesIO()
    wb.save(stream)

    progress_bar.progress(100, text="Done!")
    return stream.getvalue()


# ---------------------------------------------------------
# Streamlit App
# ---------------------------------------------------------
def main():
    st.set_page_config(page_title="AI Document Structuring Tool", layout="wide")
    st.title("üìÑ AI-Powered Document Structuring & Data Extraction")

    # --- Sidebar for API Key Input ---
    with st.sidebar:
        st.header("‚öôÔ∏è Configuration")
        st.markdown("### üîë Enter Your Groq API Key")

        # Input the API Key and store it in session state
        api_key_input = st.text_input(
            label="Groq API Key",
            placeholder="Enter your GROQ_API_KEY here",
            type="password",
            key="API_KEY_INPUT", # Use a unique key for the widget
            value=st.session_state.get("API_KEY", os.environ.get("GROQ_API_KEY", "")),
            help="Your key is used only for this session's API calls."
        )
        
        # Manually update API_KEY in session state for cross-widget access
        st.session_state["API_KEY"] = api_key_input

        client, connected = init_client(st.session_state["API_KEY"])

        if connected:
            st.success(f"API Status: Connected to GroqCloud")
        else:
            st.error("API Status: Disconnected")
            st.info("Enter a valid API key in the field above to enable the LLM.")

        st.markdown("---")
        st.caption(f"LLM Model: **{LLM_MODEL}**")

    # ------------------ Block UI if no API key ------------------
    if not connected:
        st.warning("Please enter your API key in the sidebar to activate the application.")
        return

    # ------------------ Main UI ------------------
    st.markdown("## ‚û°Ô∏è Process Workflow")

    uploaded_file = st.file_uploader("**Step 1:** Upload 'Data Input.pdf'", type=["pdf"])

    if uploaded_file:
        st.success(f"Uploaded: {uploaded_file.name}")

        # Invalidate cache if a new file is uploaded
        if uploaded_file.name != st.session_state.get('last_uploaded_file'):
             st.session_state['pdf_content'] = read_pdf_text(uploaded_file)
             st.session_state['last_uploaded_file'] = uploaded_file.name
        
        # Ensure content is in session state
        if 'pdf_content' not in st.session_state:
             st.session_state['pdf_content'] = read_pdf_text(uploaded_file)


        col1, col2 = st.columns(2)

        with col1:
            if st.button("2. üîç Preview Extracted Text", use_container_width=True):
                st.success("Text extracted successfully.")
                with st.expander("Show Raw PDF Text"):
                    st.code(st.session_state['pdf_content'][:1000] + " ...", language="text")

        with col2:
            run_extract = st.button("3. ‚ö° Run LLM Extraction", type="primary", use_container_width=True)

        st.markdown("---")

        if run_extract:
            pdf_text = st.session_state['pdf_content']
            
            if not pdf_text:
                st.warning("Cannot proceed: PDF text content is empty.")
                return

            progress = st.progress(10, text="Starting LLM Processing...")

            # Extract data
            data = extract_data_with_llm(client, pdf_text, progress)

            if data:
                st.subheader("‚úÖ Step 4: Extraction Complete")

                # Create downloadable Excel
                excel_bytes = create_excel_bytes(data, progress)
                
                # Display preview using model_dump(by_alias=True)
                st.dataframe([d.model_dump(by_alias=True) for d in data], use_container_width=True, height=300)

                st.download_button(
                    "‚¨áÔ∏è Download Structured Output.xlsx",
                    excel_bytes,
                    "Structured_Output.xlsx",
                    "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
                progress.empty()
            
if __name__ == "__main__":
    main()
